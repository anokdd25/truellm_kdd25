general:
  dataset: heart

model:
  model: meta-llama/Llama-2-7b-chat-hf   #mistralai/Mistral-7B-Instruct-v0.2
  device_map: auto

infer:
  top_k: 10
  num_return_sequences: 1
  max_length: 512

finetune:
  seed: 0
  numshot: 64
  per_device_train_batch_size: 64
  gradient_accumulation_steps: 1
  learning_rate: 0.0001
  logging_steps: 1
  num_train_epochs: 50
  max_steps: -1
  save_steps: 50
  save_total_limit: 20
  gradient_checkpointing: False
  lora_r: 32
  lora_alpha: 32
